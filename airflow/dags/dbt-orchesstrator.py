import os
import sys
import json # to load dbt's manifest.json
import pendulum # better timezone handling for DAG scheduling
from airflow import DAG #DAG object to define the workflow
from airflow.operators.bash import BashOperator # to run dbt commands/bash commands as tasks
#from airflow.sensors.external_task import ExternalTaskSensor  # to wait for tasks in other DAGs
from airflow.operators.python import PythonOperator  # to run Python functions as tasks

sys.path.append(os.path.join(os.environ['HOME'], 'myrepos/dbt-postgres-airflow/airflow/utilities'))
from helper_function import main

# Get the HOME directory of current user
HOME = os.environ['HOME']

# path to your dbt project
dbt_path = os.path.join(HOME, "myrepos/dbt-postgres-airflow/dbt/canh_dbt_proj")

# Load the dbt manifest file to get the list of models
# This file is generated by dbt when you run `dbt compile` or `dbt run`
# It contains metadata about your dbt project, including models, sources, etc.
# The manifest file is usually located in the target directory of your dbt project
manifest_path = os.path.join(dbt_path, "target/manifest.json")

with open(manifest_path) as f:
    manifest = json.load(f) #
    nodes = manifest['nodes']  # Get all models/test/seed/snapshot nodes

# Define an Airflow DAG 
with DAG(
    dag_id="dbt_orchestrator",  # this ID shows up in Airflow UI
    start_date=pendulum.today(),  # start date for the DAG to become active
    schedule="44 18 * * *",  # run the DAG daily at 5:40PM
) as dag:

    # Define a task to run the main function from helper_function.py
    fetch_weather_task = PythonOperator(
        task_id="ingest_weather_data",  # task ID
        python_callable=main
    )

    dbt_tasks = dict()  # Dictionary to hold dbt tasks/dinamically created tasks
    # Create a task for each model in the dbt project
    for node_id, node_info in nodes.items():
        if node_info['resource_type'] == 'model':  # Only create tasks for models
            dbt_tasks[node_id] = BashOperator(
                task_id=".".join(
                    [
                        node_info['resource_type'],  # e.g., model, test, seed, snapshot
                        node_info['package_name'],  # e.g., canh_dbt_proj
                        node_info['name'],  # e.g., my_model, my_test, my_seed
                    ]    

                ),  # Use the node ID as the task ID
                bash_command=(
                    f"cd {dbt_path} && "  # navigate to the dbt project directory
                    f"/home/cnguyen/.pyenv/versions/demo_dbt/bin/dbt run"  # run the models
                ),
            )

            fetch_weather_task >> dbt_tasks[node_id]  # Set the dependency to run after fetching weather data
   
    for node_id, node_info in nodes.items():
        upstream_nodes = node_info['depends_on']['nodes']  # Get the upstream nodes for the current node
        if upstream_nodes:  # If there are upstream nodes, set dependencies
            for upstream_node in upstream_nodes:
                if upstream_node in dbt_tasks:
                    # Set the upstream node to run before the current node
                    # use Airflow's bitshift to define dependency chain
                    dbt_tasks[upstream_node] >> dbt_tasks[node_id]

if __name__ == "__main__":
    dag.cli()  # Run the DAG from the command line for testing purposes